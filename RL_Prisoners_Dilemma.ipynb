{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Policy Iteration in the Repeated Prisoner's Dilemma\n",
    "\n",
    "## Part I: Build the Environment\n",
    "We have implemented the `RPDEnv` class in `rpd_env.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T11:34:12.382733Z",
     "iopub.status.busy": "2025-12-06T11:34:12.382513Z",
     "iopub.status.idle": "2025-12-06T11:34:12.684631Z",
     "shell.execute_reply": "2025-12-06T11:34:12.684300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rpd_env import RPDEnv, COOPERATE, DEFECT\n",
    "from policy_iteration import policy_iteration\n",
    "\n",
    "# Helper function to print policy nicely\n",
    "def print_policy(policy, env):\n",
    "    print(\"Optimal Policy:\")\n",
    "    for s_idx, action in enumerate(policy):\n",
    "        state = env.idx_to_state[s_idx]\n",
    "        action_str = \"C\" if action == COOPERATE else \"D\"\n",
    "        print(f\"State {state}: {action_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Define the MDP\n",
    "We will generate and report the State Space, Transition Probabilities, and Rewards for each scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T11:34:12.685928Z",
     "iopub.status.busy": "2025-12-06T11:34:12.685815Z",
     "iopub.status.idle": "2025-12-06T11:34:12.688448Z",
     "shell.execute_reply": "2025-12-06T11:34:12.688258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Memory Depth 1 ---\n",
      "\n",
      "Opponent: ALL-C\n",
      "Number of States: 4\n",
      "Sample Transitions (State 0, Action C):\n",
      "  -> State (0, 0) with prob 1.0\n",
      "Reward at State 0, Action C: 3.0\n",
      "\n",
      "Opponent: ALL-D\n",
      "Number of States: 4\n",
      "Sample Transitions (State 0, Action C):\n",
      "  -> State (0, 1) with prob 1.0\n",
      "Reward at State 0, Action C: 0.0\n",
      "\n",
      "Opponent: TFT\n",
      "Number of States: 4\n",
      "Sample Transitions (State 0, Action C):\n",
      "  -> State (0, 0) with prob 1.0\n",
      "Reward at State 0, Action C: 3.0\n",
      "\n",
      "Opponent: Imperfect-TFT\n",
      "Number of States: 4\n",
      "Sample Transitions (State 0, Action C):\n",
      "  -> State (0, 0) with prob 0.9\n",
      "  -> State (0, 1) with prob 0.1\n",
      "Reward at State 0, Action C: 2.7\n",
      "\n",
      "--- Memory Depth 2 ---\n",
      "\n",
      "Opponent: ALL-C\n",
      "Number of States: 16\n",
      "Sample Transitions (State 0, Action C):\n",
      "  -> State ((0, 0), (0, 0)) with prob 1.0\n",
      "Reward at State 0, Action C: 3.0\n",
      "\n",
      "Opponent: ALL-D\n",
      "Number of States: 16\n",
      "Sample Transitions (State 0, Action C):\n",
      "  -> State ((0, 0), (0, 1)) with prob 1.0\n",
      "Reward at State 0, Action C: 0.0\n",
      "\n",
      "Opponent: TFT\n",
      "Number of States: 16\n",
      "Sample Transitions (State 0, Action C):\n",
      "  -> State ((0, 0), (0, 0)) with prob 1.0\n",
      "Reward at State 0, Action C: 3.0\n",
      "\n",
      "Opponent: Imperfect-TFT\n",
      "Number of States: 16\n",
      "Sample Transitions (State 0, Action C):\n",
      "  -> State ((0, 0), (0, 0)) with prob 0.9\n",
      "  -> State ((0, 0), (0, 1)) with prob 0.1\n",
      "Reward at State 0, Action C: 2.7\n"
     ]
    }
   ],
   "source": [
    "strategies = ['ALL-C', 'ALL-D', 'TFT', 'Imperfect-TFT']\n",
    "memory_depths = [1, 2]\n",
    "\n",
    "for depth in memory_depths:\n",
    "    print(f\"\\n--- Memory Depth {depth} ---\")\n",
    "    for strategy in strategies:\n",
    "        print(f\"\\nOpponent: {strategy}\")\n",
    "        env = RPDEnv(opponent_strategy=strategy, memory_depth=depth)\n",
    "        P, R = env.get_mdp()\n",
    "        \n",
    "        print(f\"Number of States: {len(env.states)}\")\n",
    "        # Print a few transitions and rewards as examples\n",
    "        print(\"Sample Transitions (State 0, Action C):\")\n",
    "        for next_s in range(len(env.states)):\n",
    "            if P[0, COOPERATE, next_s] > 0:\n",
    "                print(f\"  -> State {env.idx_to_state[next_s]} with prob {P[0, COOPERATE, next_s]}\")\n",
    "        \n",
    "        print(f\"Reward at State 0, Action C: {R[0, COOPERATE]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Policy Iteration\n",
    "We have implemented the algorithm in `policy_iteration.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Experiments & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T11:34:12.702947Z",
     "iopub.status.busy": "2025-12-06T11:34:12.702830Z",
     "iopub.status.idle": "2025-12-06T11:34:12.803908Z",
     "shell.execute_reply": "2025-12-06T11:34:12.803654Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment(strategies, depths, gamma_values):\n",
    "    results = {}\n",
    "    \n",
    "    for depth in depths:\n",
    "        results[depth] = {}\n",
    "        for strategy in strategies:\n",
    "            rewards = []\n",
    "            policies = []\n",
    "            \n",
    "            env = RPDEnv(opponent_strategy=strategy, memory_depth=depth)\n",
    "            P, R = env.get_mdp()\n",
    "            \n",
    "            for gamma in gamma_values:\n",
    "                # Pass states list for potential plotting (verbose=False by default)\n",
    "                policy, V = policy_iteration(P, R, gamma=gamma, states=env.states)\n",
    "                policies.append(policy)\n",
    "                \n",
    "                # Calculate average reward over 50 episodes\n",
    "                total_reward = 0\n",
    "                num_episodes = 50\n",
    "                steps_per_episode = 50\n",
    "                \n",
    "                for _ in range(num_episodes):\n",
    "                    state_idx, _ = env.reset()\n",
    "                    episode_reward = 0\n",
    "                    for _ in range(steps_per_episode):\n",
    "                        action = policy[state_idx]\n",
    "                        next_state_idx, reward, _, _, _ = env.step(action)\n",
    "                        episode_reward += reward\n",
    "                        state_idx = next_state_idx\n",
    "                    total_reward += episode_reward\n",
    "                    \n",
    "                avg_reward = total_reward / num_episodes\n",
    "                rewards.append(avg_reward)\n",
    "                \n",
    "            results[depth][strategy] = {'rewards': rewards, 'policies': policies}\n",
    "        \n",
    "    return results\n",
    "\n",
    "# 1. The Discount Factor Analysis\n",
    "gammas = np.linspace(0.1, 0.99, 20)\n",
    "strategies = ['ALL-C', 'ALL-D', 'TFT', 'Imperfect-TFT']\n",
    "depths = [1, 2]\n",
    "\n",
    "# Run for all depths\n",
    "print(\"Running experiments for Memory Depths 1 and 2...\")\n",
    "results = run_experiment(strategies, depths, gammas)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, len(depths), figsize=(18, 6))\n",
    "\n",
    "for i, depth in enumerate(depths):\n",
    "    ax = axes[i]\n",
    "    for strategy in strategies:\n",
    "        ax.plot(gammas, results[depth][strategy]['rewards'], label=strategy)\n",
    "    ax.set_xlabel('Gamma')\n",
    "    ax.set_ylabel('Average Cumulative Reward (50 steps)')\n",
    "    ax.set_title(f'Memory Depth {depth}: Reward vs Gamma')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check when cooperation becomes optimal against TFT (Depth 1)\n",
    "print(\"\\nPolicies against TFT (Depth 1) at different gammas:\")\n",
    "tft_policies = results[1]['TFT']['policies']\n",
    "for g, p in zip(gammas, tft_policies):\n",
    "    # Check if policy is all cooperate (0)\n",
    "    is_coop = np.all(p == COOPERATE)\n",
    "    print(f\"Gamma {g:.2f}: {'All Cooperate' if is_coop else p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T11:34:12.805014Z",
     "iopub.status.busy": "2025-12-06T11:34:12.804915Z",
     "iopub.status.idle": "2025-12-06T11:34:12.845165Z",
     "shell.execute_reply": "2025-12-06T11:34:12.844963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Memory-1 vs Memory-2 against all opponents (Gamma=0.9)\n",
      "\n",
      "Opponent: ALL-C\n",
      "  Memory-1 Avg Reward: 250.0\n",
      "  Memory-2 Avg Reward: 250.0\n",
      "\n",
      "Opponent: ALL-D\n",
      "  Memory-1 Avg Reward: 50.0\n",
      "  Memory-2 Avg Reward: 50.0\n",
      "\n",
      "Opponent: TFT\n",
      "  Memory-1 Avg Reward: 150.0\n",
      "  Memory-2 Avg Reward: 150.0\n",
      "\n",
      "Opponent: Imperfect-TFT\n",
      "  Memory-1 Avg Reward: 134.94\n",
      "  Memory-2 Avg Reward: 134.76\n"
     ]
    }
   ],
   "source": [
    "# 2. Memory Depth Analysis\n",
    "print(\"Comparing Memory-1 vs Memory-2 against all opponents (Gamma=0.9)\")\n",
    "gamma = 0.9\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"\\nOpponent: {strategy}\")\n",
    "    for depth in [1, 2]:\n",
    "        env = RPDEnv(opponent_strategy=strategy, memory_depth=depth)\n",
    "        P, R = env.get_mdp()\n",
    "        policy, V = policy_iteration(P, R, gamma=gamma, states=env.states)\n",
    "        \n",
    "        # Simulation\n",
    "        total_reward = 0\n",
    "        for _ in range(50):\n",
    "            state_idx, _ = env.reset()\n",
    "            for _ in range(50):\n",
    "                action = policy[state_idx]\n",
    "                next_state_idx, reward, _, _, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                state_idx = next_state_idx\n",
    "        print(f\"  Memory-{depth} Avg Reward: {total_reward/50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T11:34:12.846309Z",
     "iopub.status.busy": "2025-12-06T11:34:12.846228Z",
     "iopub.status.idle": "2025-12-06T11:34:12.851918Z",
     "shell.execute_reply": "2025-12-06T11:34:12.851411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Noise Analysis: TFT vs Imperfect-TFT (Memory-1, Gamma=0.9)\n",
      "\n",
      "Strategy: TFT\n",
      "Optimal Policy:\n",
      "State (0, 0): C\n",
      "State (0, 1): C\n",
      "State (1, 0): C\n",
      "State (1, 1): C\n",
      "  Avg Reward: 150.0\n",
      "\n",
      "Strategy: Imperfect-TFT\n",
      "Optimal Policy:\n",
      "State (0, 0): C\n",
      "State (0, 1): C\n",
      "State (1, 0): C\n",
      "State (1, 1): C\n",
      "  Avg Reward: 135.9\n"
     ]
    }
   ],
   "source": [
    "# 3. Noise Analysis\n",
    "print(\"\\nNoise Analysis: TFT vs Imperfect-TFT (Memory-1, Gamma=0.9)\")\n",
    "gamma = 0.9\n",
    "\n",
    "for strategy in ['TFT', 'Imperfect-TFT']:\n",
    "    env = RPDEnv(opponent_strategy=strategy, memory_depth=1)\n",
    "    P, R = env.get_mdp()\n",
    "    policy, V = policy_iteration(P, R, gamma=gamma, states=env.states)\n",
    "    \n",
    "    print(f\"\\nStrategy: {strategy}\")\n",
    "    print_policy(policy, env)\n",
    "    \n",
    "    # Simulation\n",
    "    total_reward = 0\n",
    "    for _ in range(50):\n",
    "        state_idx, _ = env.reset()\n",
    "        for _ in range(50):\n",
    "            action = policy[state_idx]\n",
    "            next_state_idx, reward, _, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state_idx = next_state_idx\n",
    "    print(f\"  Avg Reward: {total_reward/50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}