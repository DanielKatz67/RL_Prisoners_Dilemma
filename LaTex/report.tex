\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Assignment 1: Policy Iteration in the Repeated Prisoner's Dilemma\\
\large RL2026A}
\author{Daniel Katz (315114991) \\ Avital Fine (208253823)}

\begin{document}
\maketitle

\begin{abstract}
This report studies optimal decision-making in the Repeated Prisoner's Dilemma (RPD) through
a full MDP formulation and tabular Policy Iteration. We implement a custom Gymnasium
environment, define transition functions and reward structures for four opponent types
(ALL-C, ALL-D, TFT, Imperfect TFT), and evaluate two observation schemes (Memory-1 and Memory-2).
We then perform Policy Iteration for varying discount factors and analyze how memory depth
and stochasticity influence optimal behavior.
\end{abstract}

\section{Introduction}
The Repeated Prisoner's Dilemma (RPD) is one of the canonical environments for studying cooperation,
strategic behavior, and temporal decision-making. In this assignment, we model the RPD as a Markov
Decision Process (MDP) fully consistent with the Gymnasium API, and examine how an optimal agent adapts
its policy against different opponent personalities. We follow the structure of the assignment
specification (RL2026A-Assignment1.pdf).

\section{Part II: MDP Definition}

\subsection{State Space}
We evaluated two observation schemes as required.

\subsubsection{Memory-1}
The agent observes only the previous round's joint actions $(a_t^{\text{agent}}, a_t^{\text{opp}})$.
Thus the state space is:
\[
S_{\text{M1}} = \{ (C,C), (C,D), (D,C), (D,D) \},
\]
with a total of 4 states.

\subsubsection{Memory-2}
The agent observes its two most recent actions and the opponent’s two most recent actions:
\[
s_t = (a_{t-1}^{A}, a_{t-1}^{O}, a_{t-2}^{A}, a_{t-2}^{O}).
\]
Since each action is binary, the total state count is:
\[
|S_{\text{M2}}| = 2^4 = 16.
\]

The full list of states (where each state is $(a_{t-1}^{A}, a_{t-1}^{O}, a_{t-2}^{A}, a_{t-2}^{O})$) is:
\begin{center}
\begin{tabular}{cccc}
(C,C,C,C) & (C,C,C,D) & (C,C,D,C) & (C,C,D,D) \\
(C,D,C,C) & (C,D,C,D) & (C,D,D,C) & (C,D,D,D) \\
(D,C,C,C) & (D,C,C,D) & (D,C,D,C) & (D,C,D,D) \\
(D,D,C,C) & (D,D,C,D) & (D,D,D,C) & (D,D,D,D)
\end{tabular}
\end{center}

Initial states follow the assignment convention:
\[
\text{M1 initial: } (C,C), \quad \text{M2 initial: } (C,C,C,C).
\]

\subsection{Actions}
\[
A = \{C, D\}.
\]

\subsection{Transition Probability Function}
The transition probability $P(s'|s,a)$ defines the probability of moving to state $s'$ given current state $s$ and agent action $a$.

\subsubsection{General Form}
The new state $s'$ is determined by shifting the history.
\begin{itemize}
    \item \textbf{Memory-1:} If $s = (a_{t-1}^A, a_{t-1}^O)$ and agent chooses $a_t^A$, the new state is $s' = (a_t^A, a_t^O)$.
    \item \textbf{Memory-2:} If $s = (a_{t-1}^A, a_{t-1}^O, a_{t-2}^A, a_{t-2}^O)$ and agent chooses $a_t^A$, the new state is $s' = (a_t^A, a_t^O, a_{t-1}^A, a_{t-1}^O)$.
\end{itemize}

In both cases, $a_t^A$ is given by the agent's choice. The opponent's action $a_t^O$ is determined by their strategy policy $\pi_{opp}(s)$.
Thus, the transition probability is:
\[
P(s'|s, a_t^A) = P(a_t^O | s).
\]

It is important to note that for any state $s'$ that does not match the history shift or implies an impossible opponent action, the probability is 0.
For example, against an \textbf{ALL-C} opponent, any transition to a state where the opponent's last action is $D$ has a probability of 0:
\[
P(s' = (\dots, D) | s, a) = 0.
\]

\subsubsection{Opponent Strategies}
\begin{itemize}
    \item \textbf{ALL-C:} $P(a_t^O=C|s) = 1$.
    \item \textbf{ALL-D:} $P(a_t^O=D|s) = 1$.
    \item \textbf{TFT:} Deterministic. Copies agent's last move.
    \[
    P(a_t^O = a_{t-1}^A | s) = 1.
    \]
    \item \textbf{Imperfect TFT:} Stochastic.
    \[
    P(a_t^O = a_{t-1}^A | s) = 0.9, \quad P(a_t^O \neq a_{t-1}^A | s) = 0.1.
    \]
\end{itemize}

\subsection{Reward Function}
The reward function $R(s,a)$ represents the \textbf{expected immediate reward} the agent receives when taking action $a$ in state $s$.
It is calculated by summing over the possible opponent actions $a^O$, weighted by their probability of occurring given the current state (history):
\[
R(s, a) = \sum_{a^O \in \{C, D\}} P(a^O | s) \cdot \text{Payoff}(a, a^O)
\]
where $\text{Payoff}(a, a^O)$ is given by the standard RPD matrix:
\[
\text{Payoff}(C,C)=3,\quad \text{Payoff}(C,D)=0,\quad \text{Payoff}(D,C)=5,\quad \text{Payoff}(D,D)=1.
\]

For deterministic opponents (ALL-C, ALL-D, TFT), $P(a^O|s)$ is either 0 or 1, so the expected reward equals the specific payoff entry.
For stochastic opponents (Imperfect TFT), the expected reward is a weighted average. For example, if Imperfect TFT has a 90\% chance of cooperating:
\[
R(s, C) = 0.9 \cdot 3 + 0.1 \cdot 0 = 2.7.
\]

\section{Part III: Policy Iteration}
We implemented classical tabular Policy Iteration with:
\begin{enumerate}
    \item \textbf{Policy Evaluation:}
    Solving $V^{\pi}$ via iterative Bellman updates until convergence.
    \item \textbf{Policy Improvement:}
    Updating $\pi(s)$ to:
    \[
        \pi'(s) = \arg\max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a)V^{\pi}(s') \right].
    \]
\end{enumerate}

Convergence was rapid across all opponents due to the small state space.

\section{Part IV: Experiments and Analysis}

\subsection{Discount Factor Analysis}
We varied $\gamma$ and computed the optimal strategy against TFT.

\[
\textbf{Cooperation becomes optimal when } \gamma \ge 0.71.
\]

\subsubsection{Why lower $\gamma$ forces Defection?}
A lower discount factor $\gamma$ means the agent values immediate rewards significantly more than future rewards.
Against TFT, cooperation yields a steady stream of $R=3$. Defection yields an immediate temptation $T=5$, but triggers punishment $P=1$ in subsequent rounds.
If the agent is "short-sighted" (low $\gamma$), the immediate gain of $5$ outweighs the long-term loss of falling from $3$ to $1$.
To find the exact threshold where cooperation becomes optimal, we utilized the \texttt{run\_analysis} function.
We iterated through a range of $\gamma$ values (from 0.1 to 0.99). For each $\gamma$, we ran Policy Iteration to find the optimal policy against a TFT opponent.
We then checked if the resulting policy was "ALL-C" (Cooperate in all states). The first $\gamma$ for which the optimal policy switched to ALL-C was identified as the threshold.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{average_reward_vs_gamma.png}
    \caption{Average Cumulative Reward vs. Discount Factor ($\gamma$) for TFT.}
    \label{fig:rewards_vs_gamma}
\end{figure}

\subsection{Memory Depth Comparison}
We measured average cumulative reward over 50 episodes (each 50 steps).
The results:

\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Opponent} & \textbf{Memory-1 Score} & \textbf{Memory-2 Score} \\
\midrule
ALL-C & 250.0 & 250.0 \\
ALL-D & 50.0 & 50.0 \\
TFT & 225.0 & 225.0 \\
Imperfect TFT & 136.02 & 135.12 \\
\bottomrule
\end{tabular}
\end{center}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{memory_depth_comparison.png}
    \caption{Comparison of Average Cumulative Rewards for Memory-1 vs. Memory-2 agents.}
    \label{fig:memory_depth_comparison}
\end{figure}



\subsubsection{Interpretation}
\begin{itemize}
    \item \textbf{ALL-C (Score 250):} The optimal policy against ALL-C is to always Defect. Since ALL-C always Cooperates, the agent receives the temptation payoff $T=5$ in every round. Over 50 rounds, the total reward is $50 \times 5 = 250$.
    \item \textbf{ALL-D (Score 50):} The optimal policy against ALL-D is to always Defect. Since ALL-D always Defects, the agent receives the punishment payoff $P=1$ in every round. Over 50 rounds, the total reward is $50 \times 1 = 50$. Cooperating would yield the sucker's payoff $S=0$, which is worse.
    \item \textbf{Memory-1 vs Memory-2 (Deterministic Opponents):} For ALL-C, ALL-D, and TFT, the scores for Memory-1 and Memory-2 are identical. This is because these opponents' strategies depend only on the immediate past (or no history at all). Having an extra step of memory (Memory-2) provides no additional predictive power or strategic advantage against them.
    \item \textbf{Imperfect TFT (Stochastic Differences):} For Imperfect TFT, the scores differ slightly (136.02 vs 135.12) despite the optimal policy being the same (Cooperate). This difference is due to the stochastic nature of the opponent (10\% noise). The variance in random outcomes across the 50 simulation episodes leads to slightly different average rewards, but statistically, the performance is equivalent.
\end{itemize}

\subsubsection{Hypothetical Opponent Where Memory-2 Helps}

We propose a history-dependent opponent called the \textbf{Two-Step Punisher (TSP)}.
The opponent's action depends on the agent's last \emph{two} actions:

\[
a^{O}_t =
\begin{cases}
C & \text{if } (a^{A}_{t-1}, a^{A}_{t-2}) = (C,C), \\[6pt]
D & \text{if } a^{A}_{t-1} = D \text{ and } a^{A}_{t-2} = C, \\[6pt]
C & \text{if } a^{A}_{t-1} = C \text{ and } a^{A}_{t-2} = D, \\[6pt]
D & \text{if } (a^{A}_{t-1}, a^{A}_{t-2}) = (D,D).
\end{cases}
\]

This opponent “punishes” a defection for exactly one turn if it was recent 
(\(a^{A}_{t-1}=D\)), and for two turns if the agent defected twice in a row.
Importantly, the opponent behaves differently for histories that \textbf{appear identical to a Memory-1 agent}.

\paragraph{Why Memory-2 is Required.}
A Memory-1 agent only observes the most recent pair \((a^{A}_{t-1}, a^{O}_{t-1})\), which collapses multiple distinct two-step histories into a single state.
For example, these two Memory-2 states:
\[
(C,C,C,C) \quad\text{and}\quad (C,C,D,C)
\]
both appear as the same Memory-1 state:
\[
(C,C),
\]
yet the opponent's next action differs depending on whether the older action was \(C\) or \(D\).
Thus, a Memory-1 agent cannot predict the opponent's behavior, whereas a Memory-2 agent can.

\subsubsection{Example Interaction (10 Rounds)}

Below we illustrate an interaction with TSP where the agent defects once at Round 5.
The opponent's behavior depends on the agent's \emph{two-step} history, not the one-step history.

\begin{center}
\begin{tabular}{cccl}
\toprule
\textbf{Round} & \textbf{Agent} & \textbf{Opponent} & \textbf{Explanation} \\
\midrule
1 & C & C & Initial cooperation \\
2 & C & C & \\
3 & C & C & \\
4 & C & C & \\
5 & \textbf{D} & C & Agent defects (temptation) \\
6 & C & \textbf{D} & Opponent punishes: last 2 actions include a D (Round 5) \\
7 & C & C & Opponent sees history $(C,D)$: forgiveness \\
8 & C & C & Cooperation restored \\
9 & C & C & \\
10 & C & C & \\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Analysis of Point Loss.}
If the agent cooperated throughout Rounds 5--7, it would have earned:
\[
3 + 3 + 3 = 9.
\]

Instead:
\[
\text{Round 5 payoff: } T = 5,\qquad
\text{Round 6 payoff: } P = 1,\qquad
\text{Round 7 payoff: } 3.
\]

Total:
\[
5 + 1 + 3 = 9.
\]

Depending on the opponent's exact punishment rule (length and severity), a defection may lead to:
\[
\text{(Temptation gain)} - \text{(Punishment loss)}
\]
which Memory-2 can correctly anticipate, while Memory-1 cannot.

\subsubsection{Why Memory-2 Strictly Outperforms Memory-1}

The TSP opponent uses two-step history, so the agent requires Memory-2 to optimally respond.

\begin{itemize}
    \item \textbf{Memory-1 Agent (Insufficient Information):}
    Only sees $(a^{A}_{t-1}, a^{O}_{t-1})$.
    It cannot distinguish whether the opponent's current defection is due to:
    \begin{itemize}
        \item a defection one turn ago, or
        \item a defection two turns ago.
    \end{itemize}
    Identical Memory-1 states lead to different opponent reactions, so the Memory-1 agent cannot build an optimal policy.
    It may incorrectly retaliate or fail to return to cooperation at the right time.

    \item \textbf{Memory-2 Agent (Full Information):}
    Observes $(a^{A}_{t-1}, a^{O}_{t-1}, a^{A}_{t-2}, a^{O}_{t-2})$ and therefore knows:
    \begin{itemize}
        \item why the punishment started,
        \item how long it will last,
        \item exactly when cooperation can be safely resumed.
    \end{itemize}
    This allows it to maintain cooperation far more consistently and avoid unnecessary punishment cycles.
\end{itemize}

\paragraph{Conclusion.}
Because the opponent's behavioral rule depends on the last \textbf{two} agent actions, a Memory-2 agent
can correctly interpret and predict the opponent's responses, while a Memory-1 agent cannot distinguish
critical histories. Consequently, Memory-2 achieves a strictly higher long-term reward against the
Two-Step Punisher opponent.

\subsection{Noise Analysis: TFT vs Imperfect TFT}
The optimal policies were:

\subsubsection*{TFT}
\[
\pi(s) = C \quad \forall s \in \{ (0,0), (0,1), (1,0), (1,1) \}
\]

\subsubsection*{Imperfect TFT}
\[
\pi(s) = C \quad \forall s
\]

\subsubsection{Interpretation}
Even with 10\% noise, cooperation remains optimal.

The noise does \emph{lower} the achievable return:
\[
\text{TFT reward: } 150.00, \qquad
\text{Imperfect TFT reward: } 136.02.
\]

But the optimal policy still prefers cooperation over mutual defection,
which would yield only 1 point per step.

Thus the agent behaves \textbf{forgivingly}: it sustains cooperation even when
occasional mistakes occur.

\section{Summary}
In this assignment, we modeled the Repeated Prisoner's Dilemma as an MDP and used Policy Iteration to find optimal strategies. Our key findings are:
\begin{itemize}
    \item \textbf{Discount Factor:} Cooperation against TFT requires a sufficient view of the future ($\gamma \ge 0.71$). Below this threshold, the agent succumbs to the temptation of immediate defection.
    \item \textbf{Memory Depth:} Against standard opponents (ALL-C, ALL-D, TFT), Memory-2 provides no advantage over Memory-1, as these strategies do not rely on long-term history.
    \item \textbf{Hypothetical Opponent:} We demonstrated that against a "Two-Step Punisher" (TSP), Memory-2 is strictly necessary. The TSP's dependence on 2-step history creates state aliasing for a Memory-1 agent, making the opponent appear unpredictable, whereas a Memory-2 agent can correctly predict and navigate the punishment rules.
    \item \textbf{Noise:} While noise (Imperfect TFT) reduces the total accumulated reward, it does not break the optimal cooperative policy. The agent learns to be forgiving rather than entering a permanent defect cycle.
\end{itemize}

\end{document}
