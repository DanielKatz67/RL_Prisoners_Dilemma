{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from rpd_env import RPDEnv, COOPERATE, DEFECT\n",
                "from policy_iteration import policy_iteration\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- 1. Discount Factor Analysis ---\")\n",
                "strategies = ['ALL-C', 'ALL-D', 'TFT', 'Imperfect-TFT']\n",
                "gammas = np.linspace(0.1, 0.99, 20)\n",
                "\n",
                "# Check when cooperation becomes optimal against TFT (Depth 1)\n",
                "env = RPDEnv(opponent_strategy='TFT', memory_depth=1)\n",
                "P, R = env.get_mdp()\n",
                "\n",
                "coop_gamma = None\n",
                "avg_rewards = []\n",
                "\n",
                "for gamma in gammas:\n",
                "    policy, _ = policy_iteration(P, R, gamma=gamma, states=env.states)\n",
                "    \n",
                "    # Simulation for plotting\n",
                "    total_reward = 0\n",
                "    for _ in range(10):\n",
                "        state_idx, _ = env.reset()\n",
                "        for _ in range(50):\n",
                "            action = policy[state_idx]\n",
                "            next_state_idx, reward, _, _, _ = env.step(action)\n",
                "            total_reward += reward\n",
                "            state_idx = next_state_idx\n",
                "    avg_rewards.append(total_reward / 10)\n",
                "\n",
                "    # Check if policy is all cooperate (0)\n",
                "    if np.all(policy == COOPERATE):\n",
                "        if coop_gamma is None:\n",
                "            coop_gamma = gamma\n",
                "        \n",
                "print(f\"Cooperation becomes optimal against TFT at Gamma >= {coop_gamma:.2f}\")\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(gammas, avg_rewards, marker='o')\n",
                "plt.title('Average Reward vs Discount Factor (against TFT)')\n",
                "plt.xlabel('Discount Factor (Gamma)')\n",
                "plt.ylabel('Average Cumulative Reward (50 steps)')\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n--- 2. Memory Depth Analysis ---\")\n",
                "gamma = 0.9\n",
                "strategies_list = []\n",
                "mem1_rewards = []\n",
                "mem2_rewards = []\n",
                "\n",
                "for strategy in strategies:\n",
                "    strategies_list.append(strategy)\n",
                "    rewards = {}\n",
                "    for depth in [1, 2]:\n",
                "        env = RPDEnv(opponent_strategy=strategy, memory_depth=depth)\n",
                "        P, R = env.get_mdp()\n",
                "        policy, _ = policy_iteration(P, R, gamma=gamma, states=env.states)\n",
                "        \n",
                "        # Simulation\n",
                "        total_reward = 0\n",
                "        for _ in range(50):\n",
                "            state_idx, _ = env.reset()\n",
                "            for _ in range(50):\n",
                "                action = policy[state_idx]\n",
                "                next_state_idx, reward, _, _, _ = env.step(action)\n",
                "                total_reward += reward\n",
                "                state_idx = next_state_idx\n",
                "        rewards[depth] = total_reward / 50\n",
                "    \n",
                "    mem1_rewards.append(rewards[1])\n",
                "    mem2_rewards.append(rewards[2])\n",
                "    \n",
                "    print(f\"Opponent: {strategy}\")\n",
                "    print(f\"  Memory-1: {rewards[1]:.2f}\")\n",
                "    print(f\"  Memory-2: {rewards[2]:.2f}\")\n",
                "\n",
                "# Plot\n",
                "x = np.arange(len(strategies_list))\n",
                "width = 0.35\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.bar(x - width/2, mem1_rewards, width, label='Memory-1')\n",
                "plt.bar(x + width/2, mem2_rewards, width, label='Memory-2')\n",
                "\n",
                "plt.xlabel('Opponent Strategy')\n",
                "plt.ylabel('Average Cumulative Reward')\n",
                "plt.title('Memory Depth Comparison')\n",
                "plt.xticks(x, strategies_list)\n",
                "plt.legend()\n",
                "plt.grid(axis='y')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n--- 3. Noise Analysis ---\")\n",
                "gamma = 0.9\n",
                "for strategy in ['TFT', 'Imperfect-TFT']:\n",
                "    env = RPDEnv(opponent_strategy=strategy, memory_depth=1)\n",
                "    P, R = env.get_mdp()\n",
                "    policy, _ = policy_iteration(P, R, gamma=gamma, states=env.states)\n",
                "    \n",
                "    print(f\"Strategy: {strategy}\")\n",
                "    print(\"Optimal Policy:\")\n",
                "    for s_idx, action in enumerate(policy):\n",
                "        state = env.idx_to_state[s_idx]\n",
                "        action_str = \"C\" if action == COOPERATE else \"D\"\n",
                "        print(f\"  State {state}: {action_str}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}