\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Assignment 1: Policy Iteration in the Repeated Prisoner's Dilemma\\
\large RL2026A}
\author{Daniel Katz (315114991) \\ Avital Fine (208253823)}

\begin{document}
\maketitle

\begin{abstract}
This report studies optimal decision-making in the Repeated Prisoner's Dilemma (RPD) through
a full MDP formulation and tabular Policy Iteration. We implement a custom Gymnasium
environment, define transition functions and reward structures for four opponent types
(ALL-C, ALL-D, TFT, Imperfect TFT), and evaluate two observation schemes (Memory-1 and Memory-2).
We then perform Policy Iteration for varying discount factors and analyze how memory depth
and stochasticity influence optimal behavior.
\end{abstract}

\section{Introduction}
The Repeated Prisoner's Dilemma (RPD) is one of the canonical environments for studying cooperation,
strategic behavior, and temporal decision-making. In this assignment, we model the RPD as a Markov
Decision Process (MDP) fully consistent with the Gymnasium API, and examine how an optimal agent adapts
its policy against different opponent personalities. We follow the structure of the assignment
specification (RL2026A-Assignment1.pdf).

\section{Part II: MDP Definition}

\subsection{State Space}
We evaluated two observation schemes as required.

\subsubsection{Memory-1}
The agent observes only the previous round's joint actions $(a_t^{\text{agent}}, a_t^{\text{opp}})$.
Thus the state space is:
\[
S_{\text{M1}} = \{ (C,C), (C,D), (D,C), (D,D) \},
\]
with a total of 4 states.

\subsubsection{Memory-2}
The agent observes its two most recent actions and the opponentâ€™s two most recent actions:
\[
s_t = (a_{t-1}^{A}, a_{t-1}^{O}, a_{t-2}^{A}, a_{t-2}^{O}).
\]
Since each action is binary, the total state count is:
\[
|S_{\text{M2}}| = 2^4 = 16.
\]

The full list of states (where each state is $(a_{t-1}^{A}, a_{t-1}^{O}, a_{t-2}^{A}, a_{t-2}^{O})$) is:
\begin{center}
\begin{tabular}{cccc}
(C,C,C,C) & (C,C,C,D) & (C,C,D,C) & (C,C,D,D) \\
(C,D,C,C) & (C,D,C,D) & (C,D,D,C) & (C,D,D,D) \\
(D,C,C,C) & (D,C,C,D) & (D,C,D,C) & (D,C,D,D) \\
(D,D,C,C) & (D,D,C,D) & (D,D,D,C) & (D,D,D,D)
\end{tabular}
\end{center}

Initial states follow the assignment convention:
\[
\text{M1 initial: } (C,C), \quad \text{M2 initial: } (C,C,C,C).
\]

\subsection{Actions}
\[
A = \{C, D\}.
\]

\subsection{Transition Probability Function}
The transition probability $P(s'|s,a)$ defines the probability of moving to state $s'$ given current state $s$ and agent action $a$.

\subsubsection{General Form}
The new state $s'$ is determined by shifting the history.
\begin{itemize}
    \item \textbf{Memory-1:} If $s = (a_{t-1}^A, a_{t-1}^O)$ and agent chooses $a_t^A$, the new state is $s' = (a_t^A, a_t^O)$.
    \item \textbf{Memory-2:} If $s = (a_{t-1}^A, a_{t-1}^O, a_{t-2}^A, a_{t-2}^O)$ and agent chooses $a_t^A$, the new state is $s' = (a_t^A, a_t^O, a_{t-1}^A, a_{t-1}^O)$.
\end{itemize}

In both cases, $a_t^A$ is given by the agent's choice. The opponent's action $a_t^O$ is determined by their strategy policy $\pi_{opp}(s)$.
Thus, the transition probability is:
\[
P(s'|s, a_t^A) = P(a_t^O | s).
\]

It is important to note that for any state $s'$ that does not match the history shift or implies an impossible opponent action, the probability is 0.
For example, against an \textbf{ALL-C} opponent, any transition to a state where the opponent's last action is $D$ has a probability of 0:
\[
P(s' = (\dots, D) | s, a) = 0.
\]

\subsubsection{Opponent Strategies}
\begin{itemize}
    \item \textbf{ALL-C:} $P(a_t^O=C|s) = 1$.
    \item \textbf{ALL-D:} $P(a_t^O=D|s) = 1$.
    \item \textbf{TFT:} Deterministic. Copies agent's last move.
    \[
    P(a_t^O = a_{t-1}^A | s) = 1.
    \]
    \item \textbf{Imperfect TFT:} Stochastic.
    \[
    P(a_t^O = a_{t-1}^A | s) = 0.9, \quad P(a_t^O \neq a_{t-1}^A | s) = 0.1.
    \]
\end{itemize}

\subsection{Reward Function}
The reward function $R(s,a)$ represents the \textbf{expected immediate reward} the agent receives when taking action $a$ in state $s$.
It is calculated by summing over the possible opponent actions $a^O$, weighted by their probability of occurring given the current state (history):
\[
R(s, a) = \sum_{a^O \in \{C, D\}} P(a^O | s) \cdot \text{Payoff}(a, a^O)
\]
where $\text{Payoff}(a, a^O)$ is given by the standard RPD matrix:
\[
\text{Payoff}(C,C)=3,\quad \text{Payoff}(C,D)=0,\quad \text{Payoff}(D,C)=5,\quad \text{Payoff}(D,D)=1.
\]

For deterministic opponents (ALL-C, ALL-D, TFT), $P(a^O|s)$ is either 0 or 1, so the expected reward equals the specific payoff entry.
For stochastic opponents (Imperfect TFT), the expected reward is a weighted average. For example, if Imperfect TFT has a 90\% chance of cooperating:
\[
R(s, C) = 0.9 \cdot 3 + 0.1 \cdot 0 = 2.7.
\]

\section{Part III: Policy Iteration}
We implemented classical tabular Policy Iteration with:
\begin{enumerate}
    \item \textbf{Policy Evaluation:}
    Solving $V^{\pi}$ via iterative Bellman updates until convergence.
    \item \textbf{Policy Improvement:}
    Updating $\pi(s)$ to:
    \[
        \pi'(s) = \arg\max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a)V^{\pi}(s') \right].
    \]
\end{enumerate}

Convergence was rapid across all opponents due to the small state space.

\section{Part IV: Experiments and Analysis}

\subsection{Discount Factor Analysis}
We varied $\gamma$ and computed the optimal strategy against TFT.

\[
\textbf{Cooperation becomes optimal when } \gamma \ge 0.71.
\]

\subsubsection{Why lower $\gamma$ forces Defection?}
A lower discount factor $\gamma$ means the agent values immediate rewards significantly more than future rewards.
Against TFT, cooperation yields a steady stream of $R=3$. Defection yields an immediate temptation $T=5$, but triggers punishment $P=1$ in subsequent rounds.
If the agent is "short-sighted" (low $\gamma$), the immediate gain of $5$ outweighs the long-term loss of falling from $3$ to $1$.
To find the exact threshold where cooperation becomes optimal, we utilized the \texttt{run\_analysis} function.
We iterated through a range of $\gamma$ values (from 0.1 to 0.99). For each $\gamma$, we ran Policy Iteration to find the optimal policy against a TFT opponent.
We then checked if the resulting policy was "ALL-C" (Cooperate in all states). The first $\gamma$ for which the optimal policy switched to ALL-C was identified as the threshold.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{average_reward_vs_gamma.png}
    \caption{Average Cumulative Reward vs. Discount Factor ($\gamma$) for TFT.}
    \label{fig:rewards_vs_gamma}
\end{figure}

\subsection{Memory Depth Comparison}
We measured average cumulative reward over 50 episodes (each 50 steps).
The results:

\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Opponent} & \textbf{Memory-1 Score} & \textbf{Memory-2 Score} \\
\midrule
ALL-C & 250.0 & 250.0 \\
ALL-D & 50.0 & 50.0 \\
TFT & 225.0 & 225.0 \\
Imperfect TFT & 136.02 & 135.12 \\
\bottomrule
\end{tabular}
\end{center}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{memory_depth_comparison.png}
    \caption{Comparison of Average Cumulative Rewards for Memory-1 vs. Memory-2 agents.}
    \label{fig:memory_depth_comparison}
\end{figure}



\subsubsection{Interpretation}
\begin{itemize}
    \item \textbf{ALL-C (Score 250):} The optimal policy against ALL-C is to always Defect. Since ALL-C always Cooperates, the agent receives the temptation payoff $T=5$ in every round. Over 50 rounds, the total reward is $50 \times 5 = 250$.
    \item \textbf{ALL-D (Score 50):} The optimal policy against ALL-D is to always Defect. Since ALL-D always Defects, the agent receives the punishment payoff $P=1$ in every round. Over 50 rounds, the total reward is $50 \times 1 = 50$. Cooperating would yield the sucker's payoff $S=0$, which is worse.
    \item \textbf{Memory-1 vs Memory-2 (Deterministic Opponents):} For ALL-C, ALL-D, and TFT, the scores for Memory-1 and Memory-2 are identical. This is because these opponents' strategies depend only on the immediate past (or no history at all). Having an extra step of memory (Memory-2) provides no additional predictive power or strategic advantage against them.
    \item \textbf{Imperfect TFT (Stochastic Differences):} For Imperfect TFT, the scores differ slightly (136.02 vs 135.12) despite the optimal policy being the same (Cooperate). This difference is due to the stochastic nature of the opponent (10\% noise). The variance in random outcomes across the 50 simulation episodes leads to slightly different average rewards, but statistically, the performance is equivalent.
\end{itemize}

\subsubsection{Hypothetical Opponent Where Memory-2 Helps}
Consider:
\[
\textbf{Grim-Trigger-2:} \text{cooperates unless you defected in either of the last 2 turns}.
\]
A Memory-1 agent cannot distinguish:
\[
(D,C) \text{ caused by a one-time defection} \quad \text{vs.} \quad (D,C) \text{ caused two turns ago}.
\]
A Memory-2 agent can track the full two-step history and avoid triggering punishment unnecessarily.

Thus Memory-2 strictly outperforms Memory-1.

\subsubsection{Example Interaction (10 Rounds)}
The following table demonstrates how "Grim-Trigger-2" reacts to a single defection by the agent at Round 5. The opponent punishes for exactly 2 rounds (6 and 7) before returning to cooperation.

\begin{center}
\begin{tabular}{cccl}
\toprule
\textbf{Round} & \textbf{Agent} & \textbf{Opponent} & \textbf{Explanation} \\
\midrule
1 & C & C & Initial cooperation \\
2 & C & C & \\
3 & C & C & \\
4 & C & C & \\
5 & \textbf{D} & C & \textbf{Agent defects (Exploit)} \\
6 & C & \textbf{D} & Opponent sees D in last 2 turns (Round 5) \\
7 & C & \textbf{D} & Opponent sees D in last 2 turns (Round 5) \\
8 & C & C & Last 2 turns (6, 7) were C by Agent $\rightarrow$ Forgive \\
9 & C & C & \\
10 & C & C & \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Analysis of Point Loss:}
In rounds 5, 6, and 7, the agent could have received 3 points each round by cooperating ($3 \times 3 = 9$).
However, since the agent decided to Defect in Round 5, it received the temptation payoff of 5. In the subsequent two rounds (6 and 7), it faced punishment. Assuming the agent receives 1 point per punishment round (e.g., if it also defects or if we consider the penalty value), the total for these three rounds is $5 + 1 + 1 = 7$.
Thus, by defecting, the agent lost a total of 2 points ($9 - 7 = 2$) over this sequence.

\subsubsection{Why Memory-2 is Necessary}
The "Grim-Trigger-2" opponent bases its decision on the last \textbf{two} rounds of history.
\begin{itemize}
    \item \textbf{Memory-1 Agent (Insufficient Information):} This agent only "sees" the previous round. It is effectively blind to the event (defection 2 turns ago) that is causing the opponent's current behavior. To a Memory-1 agent, the opponent appears unpredictable because identical 1-step histories (e.g., "I Cooperated, Opponent Defected") can lead to different outcomes depending on the unobserved 2nd step.
    \item \textbf{Memory-2 Agent (Complete Information):} This agent has the same memory horizon as the opponent. It can see the full cause-and-effect relationship. It knows exactly when the punishment started and, more importantly, can predict exactly when it will end.
\end{itemize}
Because the Memory-2 agent has the full context, it can confidently "wait out" the punishment and return to cooperation. The Memory-1 agent, lacking this context, cannot distinguish between a temporary punishment and a permanent defection, leading to suboptimal decisions.

\subsection{Noise Analysis: TFT vs Imperfect TFT}
The optimal policies were:

\subsubsection*{TFT}
\[
\pi(s) = C \quad \forall s \in \{ (0,0), (0,1), (1,0), (1,1) \}
\]

\subsubsection*{Imperfect TFT}
\[
\pi(s) = C \quad \forall s
\]

\subsubsection{Interpretation}
Even with 10\% noise, cooperation remains optimal.

The noise does \emph{lower} the achievable return:
\[
\text{TFT reward: } 150.00, \qquad
\text{Imperfect TFT reward: } 136.02.
\]

But the optimal policy still prefers cooperation over mutual defection,
which would yield only 1 point per step.

Thus the agent behaves \textbf{forgivingly}: it sustains cooperation even when
occasional mistakes occur.

\end{document}
