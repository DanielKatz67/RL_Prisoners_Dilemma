\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Assignment 1: Policy Iteration in the Repeated Prisoner's Dilemma\\
\large RL2026A}
\author{Daniel Katz (315114991) \\ Avital Fine (208253823)}

\begin{document}
\maketitle

\begin{abstract}
This report studies optimal decision-making in the Repeated Prisoner's Dilemma (RPD) through
a full MDP formulation and tabular Policy Iteration. We implement a custom Gymnasium
environment, define transition functions and reward structures for four opponent types
(ALL-C, ALL-D, TFT, Imperfect TFT), and evaluate two observation schemes (Memory-1 and Memory-2).
We then perform Policy Iteration for varying discount factors and analyze how memory depth
and stochasticity influence optimal behavior.
\end{abstract}

\section{Introduction}
The Repeated Prisoner's Dilemma (RPD) is one of the canonical environments for studying cooperation,
strategic behavior, and temporal decision-making. In this assignment, we model the RPD as a Markov
Decision Process (MDP) fully consistent with the Gymnasium API, and examine how an optimal agent adapts
its policy against different opponent personalities.

\section{Part II: MDP Definition}

\subsection{Actions}
\[
A = \{C, D\},
\]
where $C$ denotes \textbf{Cooperate} and $D$ denotes \textbf{Defect}.

\subsection{State Space}
We evaluated two observation schemes as required.

\subsubsection{Memory-1}
The agent observes only the previous round's joint actions $(a_t^{\text{agent}}, a_t^{\text{opp}})$.
Thus the state space is:
\[
S_{\text{M1}} = \{ (C,C), (C,D), (D,C), (D,D) \},
\]
with a total of 4 states. The initial state is $(C,C)$.

\subsubsection{Memory-2}
The agent observes its two most recent actions and the opponent’s two most recent actions:
\[
s_t = ((a_{t-2}^{A}, a_{t-2}^{O}), (a_{t-1}^{A}, a_{t-1}^{O})).
\]
Since each action is binary, the total state count is:
\[
|S_{\text{M2}}| = 2^4 = 16.
\]

The full list of states (where each state is $((a_{t-2}^{A}, a_{t-2}^{O}), (a_{t-1}^{A}, a_{t-1}^{O}))$) is:
\begin{center}
\begin{tabular}{cc}
((C,C), (C,C)) & ((C,C), (C,D)) \\
((C,C), (D,C)) & ((C,C), (D,D)) \\
((C,D), (C,C)) & ((C,D), (C,D)) \\
((C,D), (D,C)) & ((C,D), (D,D)) \\
((D,C), (C,C)) & ((D,C), (C,D)) \\
((D,C), (D,C)) & ((D,C), (D,D)) \\
((D,D), (C,C)) & ((D,D), (C,D)) \\
((D,D), (D,C)) & ((D,D), (D,D))
\end{tabular}
\end{center}
The initial state is $((C,C), (C,C))$.

\subsection{Transition Probability Function}
The transition probability $P(s'|s,a)$ defines the probability of moving to state $s'$ given current state $s$ and agent action $a$.

\subsubsection{General Form}
The new state $s'$ is determined by shifting the history.
\begin{itemize}
    \item \textbf{Memory-1:} If $s = (a_{t-1}^A, a_{t-1}^O)$ and agent chooses $a_t^A$, the new state is $s' = (a_t^A, a_t^O)$.
    \item \textbf{Memory-2:} If $s = ((a_{t-2}^A, a_{t-2}^O), (a_{t-1}^A, a_{t-1}^O))$ and agent chooses $a_t^A$, the new state is $s' = ((a_{t-1}^A, a_{t-1}^O), (a_t^A, a_t^O))$.
\end{itemize}

In both cases, $a_t^A$ is given by the agent's choice. The opponent's action $a_t^O$ is determined by their strategy policy $\pi_{opp}(s)$.
Thus, the transition probability is:
\[
P(s'|s, a_t^A) = P(a_t^O | s).
\]

It is important to note that for any state $s'$ that does not match the history shift or implies an impossible opponent action, the probability is 0.
For example, against an \textbf{ALL-C} opponent, any transition to a state where the opponent's last action is $D$ has a probability of 0:
\[
P(s' = (\dots, D) | s, a) = 0.
\]

\subsubsection{Opponent Strategies}
\begin{itemize}
    \item \textbf{ALL-C:} $P(a_t^O=C|s) = 1$.
    \item \textbf{ALL-D:} $P(a_t^O=D|s) = 1$.
    \item \textbf{TFT:} Deterministic. Copies agent's last move.
    \[
    P(a_t^O = a_{t-1}^A | s) = 1.
    \]
    \item \textbf{Imperfect TFT:} Stochastic.
    \[
    P(a_t^O = a_{t-1}^A | s) = 0.9, \quad P(a_t^O \neq a_{t-1}^A | s) = 0.1.
    \]
\end{itemize}

\subsection{Reward Function}
The reward function $R(s,a)$ represents the \textbf{expected immediate reward} the agent receives when taking action $a$ in state $s$.
It is calculated by summing over the possible opponent actions $a^O$, weighted by their probability of occurring given the current state (history):
\[
R(s, a) = \sum_{a^O \in \{C, D\}} P(a^O | s) \cdot \text{Payoff}(a, a^O)
\]
where $\text{Payoff}(a, a^O)$ is given by the standard RPD matrix:
\[
\text{Payoff}(C,C)=3,\quad \text{Payoff}(C,D)=0,\quad \text{Payoff}(D,C)=5,\quad \text{Payoff}(D,D)=1.
\]

For deterministic opponents (ALL-C, ALL-D, TFT), $P(a^O|s)$ is either 0 or 1, so the expected reward equals the specific payoff entry.
For stochastic opponents (Imperfect TFT), the expected reward is a weighted average. For example, if Imperfect TFT has a 90\% chance of cooperating:
\[
R(s, C) = 0.9 \cdot 3 + 0.1 \cdot 0 = 2.7.
\]

\section{Part III: Policy Iteration}
We implemented classical tabular Policy Iteration with:
\begin{enumerate}
    \item \textbf{Policy Evaluation:}
    Solving $V^{\pi}$ via iterative Bellman updates until convergence.
    \item \textbf{Policy Improvement:}
    Updating $\pi(s)$ to:
    \[
        \pi'(s) = \arg\max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a)V^{\pi}(s') \right].
    \]
\end{enumerate}

Convergence was rapid across all opponents due to the small state space.

\section{Part IV: Experiments and Analysis}

\subsection{Discount Factor Analysis}
We varied $\gamma$ and computed the optimal strategy against different opponents.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{average_cumulative_reward_vs_gamma_by_opponent.png}
    \caption{Average Cumulative Reward vs. Discount Factor ($\gamma$) for different opponent strategies.}
    \label{fig:rewards_vs_gamma}
\end{figure}

\subsubsection{When and Why Cooperation Becomes Optimal}
As shown in Figure \ref{fig:rewards_vs_gamma}, for the \textbf{TFT} opponent, there is a clear phase transition.
At low $\gamma$ values (e.g., 0.1, 0.5), the agent prefers Defection because it values the immediate temptation payoff ($T=5$) more than the long-term stream of cooperation rewards ($R=3$).
However, as $\gamma$ increases (e.g., 0.9, 0.99), the agent becomes "far-sighted" enough to value the future rewards of mutual cooperation. It realizes that the long-term penalty of getting punished by TFT outweighs the short-term gain of defecting, making \textbf{Cooperation} the optimal policy.

For \textbf{ALL-C} and \textbf{ALL-D}, the optimal policy is always to Defect regardless of $\gamma$, as seen by the constant reward lines. Against ALL-C, defecting exploits the opponent forever. Against ALL-D, defecting is the only defense against being exploited.

\subsection{Memory Depth Comparison}
We measured average cumulative reward over 50 episodes (each 50 steps).
The results:

\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Opponent} & \textbf{Memory-1 Score} & \textbf{Memory-2 Score} \\
\midrule
ALL-C & 250.0 & 250.0 \\
ALL-D & 50.0 & 50.0 \\
TFT & 225.0 & 225.0 \\
Imperfect TFT & 136.02 & 135.12 \\
\bottomrule
\end{tabular}
\end{center}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{memory_depth_comparison.png}
    \caption{Comparison of Average Cumulative Rewards for Memory-1 vs. Memory-2 agents.}
    \label{fig:memory_depth_comparison}
\end{figure}



\subsubsection{Interpretation}
\begin{itemize}
    \item \textbf{ALL-C (Score 250):} The optimal policy against ALL-C is to always Defect. Since ALL-C always Cooperates, the agent receives the temptation payoff $T=5$ in every round. Over 50 rounds, the total reward is $50 \times 5 = 250$.
    \item \textbf{ALL-D (Score 50):} The optimal policy against ALL-D is to always Defect. Since ALL-D always Defects, the agent receives the punishment payoff $P=1$ in every round. Over 50 rounds, the total reward is $50 \times 1 = 50$. Cooperating would yield the sucker's payoff $S=0$, which is worse.
    \item \textbf{Memory-1 vs Memory-2 (Deterministic Opponents):} For ALL-C, ALL-D, and TFT, the scores for Memory-1 and Memory-2 are identical. This is because these opponents' strategies depend only on the immediate past (or no history at all). Having an extra step of memory (Memory-2) provides no additional predictive power or strategic advantage against them.
    \item \textbf{Imperfect TFT (Stochastic Differences):} For Imperfect TFT, the scores differ slightly (136.02 vs 135.12) despite the optimal policy being the same (Cooperate). This difference is due to the stochastic nature of the opponent (10\% noise). The variance in random outcomes across the 50 simulation episodes leads to slightly different average rewards, but statistically, the performance is equivalent.
\end{itemize}

\subsubsection{Hypothetical Opponent Where Memory-2 Helps}

We propose a history-dependent opponent called the \textbf{Two-Step Punisher (TSP)}.
The opponent's action depends on the agent's last \emph{two} actions:

\[
a^{O}_t =
\begin{cases}
C & \text{if } (a^{A}_{t-1}, a^{A}_{t-2}) = (C,C), \\[6pt]
D & \text{if } a^{A}_{t-1} = D \text{ and } a^{A}_{t-2} = C, \\[6pt]
C & \text{if } a^{A}_{t-1} = C \text{ and } a^{A}_{t-2} = D, \\[6pt]
D & \text{if } (a^{A}_{t-1}, a^{A}_{t-2}) = (D,D).
\end{cases}
\]

This opponent “punishes” a defection for exactly one turn if it was recent 
(\(a^{A}_{t-1}=D\)), and for two turns if the agent defected twice in a row.
Importantly, the opponent behaves differently for histories that \textbf{appear identical to a Memory-1 agent}.

\paragraph{Why Memory-2 is Required.}
A Memory-1 agent only observes the most recent pair \((a^{A}_{t-1}, a^{O}_{t-1})\), which collapses multiple distinct two-step histories into a single state.
For example, these two Memory-2 states:
\[
((C,C),(C,C)) \quad\text{and}\quad ((D,C),(C,C))
\]
both appear as the same Memory-1 state:
\[
(C,C),
\]
yet the opponent's next action differs depending on whether the older action was \(C\) or \(D\).
Thus, a Memory-1 agent cannot predict the opponent's behavior, whereas a Memory-2 agent can.

\subsubsection{Example Interaction (10 Rounds)}

Below we illustrate an interaction with TSP where the agent defects once at Round 5.
The opponent's behavior depends on the agent's \emph{two-step} history, not the one-step history.

\begin{center}
\begin{tabular}{cccl}
\toprule
\textbf{Round} & \textbf{Agent} & \textbf{Opponent} & \textbf{Explanation} \\
\midrule
1 & C & C & Initial cooperation \\
2 & C & C & \\
3 & C & C & \\
4 & C & C & \\
5 & \textbf{D} & C & Agent defects (temptation) \\
6 & C & \textbf{D} & Opponent punishes: last 2 actions include a D (Round 5) \\
7 & C & C & Opponent sees history $(C,D)$: forgiveness \\
8 & C & C & Cooperation restored \\
9 & C & C & \\
10 & C & C & \\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Analysis of Point Loss.}
If the agent cooperated throughout Rounds 5--7, it would have earned:
\[
3 + 3 + 3 = 9.
\]

Instead:
\[
\text{Round 5 payoff: } T = 5,\qquad
\text{Round 6 payoff: } P = 1,\qquad
\text{Round 7 payoff: } 3.
\]

Total:
\[
5 + 1 + 3 = 9.
\]

Depending on the opponent's exact punishment rule (length and severity), a defection may lead to:
\[
\text{(Temptation gain)} - \text{(Punishment loss)}
\]
which Memory-2 can correctly anticipate, while Memory-1 cannot.

\subsubsection{Why Memory-2 Strictly Outperforms Memory-1}

The TSP opponent uses two-step history, so the agent requires Memory-2 to optimally respond.

\begin{itemize}
    \item \textbf{Memory-1 Agent (Insufficient Information):}
    Only sees $(a^{A}_{t-1}, a^{O}_{t-1})$.
    It cannot distinguish whether the opponent's current defection is due to:
    \begin{itemize}
        \item a defection one turn ago, or
        \item a defection two turns ago.
    \end{itemize}
    Identical Memory-1 states lead to different opponent reactions, so the Memory-1 agent cannot build an optimal policy.
    It may incorrectly retaliate or fail to return to cooperation at the right time.

    \item \textbf{Memory-2 Agent (Full Information):}
    Observes $((a^{A}_{t-2}, a^{O}_{t-2}), (a^{A}_{t-1}, a^{O}_{t-1}))$ and therefore knows:
    \begin{itemize}
        \item why the punishment started,
        \item how long it will last,
        \item exactly when cooperation can be safely resumed.
    \end{itemize}
    This allows it to maintain cooperation far more consistently and avoid unnecessary punishment cycles.
\end{itemize}

\paragraph{Conclusion.}
Because the opponent's behavioral rule depends on the last \textbf{two} agent actions, a Memory-2 agent
can correctly interpret and predict the opponent's responses, while a Memory-1 agent cannot distinguish
critical histories. Consequently, Memory-2 achieves a strictly higher long-term reward against the
Two-Step Punisher opponent.

\subsection{Noise Analysis: TFT vs Imperfect TFT}
The optimal policies were:

\subsubsection*{TFT}
\[
\pi(s) = C \quad \forall s
\]

\subsubsection*{Imperfect TFT}
\[
\pi(s) = C \quad \forall s
\]

\subsubsection{Interpretation}
Even with 10\% noise, cooperation remains optimal.

The noise does \emph{lower} the achievable return:
\[
\text{TFT reward: } 150.00, \qquad
\text{Imperfect TFT reward: } 136.02.
\]

But the optimal policy still prefers cooperation over mutual defection,
which would yield only 1 point per step.

Thus the agent behaves \textbf{forgivingly}: it sustains cooperation even when
occasional mistakes occur.

\section{Summary}
This report presented a comprehensive MDP formulation of the Repeated Prisoner's Dilemma. By implementing Policy Iteration, we derived optimal strategies against both deterministic and stochastic opponents.

We found that:
\begin{itemize}
    \item \textbf{Strategic Foresight:} Cooperation emerges only when the discount factor is high enough, proving that short-term greed is suboptimal in the long run.
    \item \textbf{Memory Requirements:} While Memory-1 is sufficient for standard strategies, we proved that complex, history-dependent opponents like the \textbf{Two-Step Punisher} require Memory-2. The \textbf{nested state representation} of Memory-2 allows the agent to distinguish histories that appear identical to a Memory-1 agent, enabling optimal punishment avoidance.
    \item \textbf{Resilience:} The optimal policy remains cooperative even against a noisy \textbf{Imperfect TFT} opponent, demonstrating that forgiveness is a key component of robust long-term strategy.
\end{itemize}

\end{document}

